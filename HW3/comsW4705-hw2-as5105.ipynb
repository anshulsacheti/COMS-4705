{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 2\n",
    "\n",
    "This assignment is worth 100 points. Please submit by Friday, July 21st at 11:59pm. \n",
    "\n",
    "### Question 1\n",
    "\n",
    "Using `spacy`, list all the entities in the following text (15 points):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Charles Bingley, Netherfield Park, Longbourn, five, Jane, Elizabeth, Mary, Kitty, Lydia—, Bennet, Bennet, Bingley, Bennets, Bingley, Jane, Darcy, Elizabeth)\n"
     ]
    }
   ],
   "source": [
    "plot = \"The news that a wealthy young gentleman named Charles Bingley has rented the manor of Netherfield Park causes a great stir in the nearby village of Longbourn, especially in the Bennet household. The Bennets have five unmarried daughters—from oldest to youngest, Jane, Elizabeth, Mary, Kitty, and Lydia—and Mrs. Bennet is desperate to see them all married. After Mr. Bennet pays a social visit to Mr. Bingley, the Bennets attend a ball at which Mr. Bingley is present. He is taken with Jane and spends much of the evening dancing with her. His close friend, Mr. Darcy, is less pleased with the evening and haughtily refuses to dance with Elizabeth, which makes everyone view him as arrogant and obnoxious.\"\n",
    "import spacy\n",
    "\n",
    "english_model = spacy.load('en')\n",
    "d = english_model(plot)\n",
    "print(d.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result1: (S the/DT (NP money/NN market/NN fund/NN))\n",
      "Result2: (S the/DT risky/JJ (NP money/NN market/NN fund/NN))\n",
      "Result3: (S\n",
      "  the/DT\n",
      "  little/JJ\n",
      "  yellow/JJ\n",
      "  (NP dog/NN)\n",
      "  barked/VBD\n",
      "  at/IN\n",
      "  the/DT\n",
      "  (NP cat/NN owner/NN))\n"
     ]
    }
   ],
   "source": [
    "sentence1 = [(\"the\", \"DT\"), (\"money\", \"NN\"), (\"market\", \"NN\"), (\"fund\", \"NN\")]\n",
    "sentence2= [(\"the\", \"DT\"), (\"risky\", \"JJ\"), (\"money\", \"NN\"), (\"market\", \"NN\"), (\"fund\", \"NN\")]\n",
    "sentence3 = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"), (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"), (\"the\", \"DT\"), (\"cat\", \"NN\"), (\"owner\", \"NN\")]\n",
    "\n",
    "another_pattern = \"NP: {<NN>+}\"\n",
    "\n",
    "import nltk \n",
    "NPAnotherChunker = nltk.RegexpParser(another_pattern) \n",
    "\n",
    "result1 = NPAnotherChunker.parse(sentence1) \n",
    "print(\"Result1: %s\" % result1)\n",
    "result2 = NPAnotherChunker.parse(sentence2) \n",
    "print(\"Result2: %s\" % result2)\n",
    "result3 = NPAnotherChunker.parse(sentence3) \n",
    "print(\"Result3: %s\" % result3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Let's consider another tag pattern, another_pattern.  This will chunk all the consecutive nouns.  Create a chunk parser.- call it `NPAnotherChunker`.  Now using this, parse `sentence1`, `sentence2` and `setence3` - call these `result1`, `result2` and `result3`, respectively.  You can see your results by using print or draw. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result1: (S (NP the/DT money/NN market/NN fund/NN))\n",
      "Result2: (S (NP the/DT risky/JJ money/NN market/NN fund/NN))\n",
      "Result3: (S\n",
      "  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
      "  barked/VBD\n",
      "  at/IN\n",
      "  (NP the/DT cat/NN owner/NN))\n"
     ]
    }
   ],
   "source": [
    "sentence1 = [(\"the\", \"DT\"), (\"money\", \"NN\"), (\"market\", \"NN\"), (\"fund\", \"NN\")]\n",
    "sentence2= [(\"the\", \"DT\"), (\"risky\", \"JJ\"), (\"money\", \"NN\"), (\"market\", \"NN\"), (\"fund\", \"NN\")]\n",
    "sentence3 = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"), (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"), (\"the\", \"DT\"), (\"cat\", \"NN\"), (\"owner\", \"NN\")]\n",
    "\n",
    "tag_pattern1 = \"NP: {<DT>?<JJ>*<NN>+}\"\n",
    "\n",
    "import nltk \n",
    "NPAnotherChunker = nltk.RegexpParser(tag_pattern1) \n",
    "result1 = NPAnotherChunker.parse(sentence1) \n",
    "print(\"Result1: %s\" % result1)\n",
    "result2 = NPAnotherChunker.parse(sentence2) \n",
    "print(\"Result2: %s\" % result2)\n",
    "result3 = NPAnotherChunker.parse(sentence3) \n",
    "print(\"Result3: %s\" % result3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The tag pattern  from the lecture `<DT>?<JJ>*<NN>` will parse sentence3 into 3 separate noun phrases.  \" the little yellow dog\", \" the cat\" and \"owner\".  Create a tag pattern that will chunk the determiner, adjectives and any number of consecutive nouns. - call this `tag_pattern1`  I.e. will chunk sentence3 to \"the little yellow dog\" and \"the cat owner\".  (hint: look at `another_pattern`) (10 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result1: (S (NP the/DT money/NN market/NN fund/NN))\n",
      "Result2: (S the/DT risky/JJ money/NN market/NN fund/NN)\n",
      "Result3: (S\n",
      "  the/DT\n",
      "  little/JJ\n",
      "  yellow/JJ\n",
      "  dog/NN\n",
      "  barked/VBD\n",
      "  at/IN\n",
      "  (NP the/DT cat/NN owner/NN))\n"
     ]
    }
   ],
   "source": [
    "sentence1 = [(\"the\", \"DT\"), (\"money\", \"NN\"), (\"market\", \"NN\"), (\"fund\", \"NN\")]\n",
    "sentence2= [(\"the\", \"DT\"), (\"risky\", \"JJ\"), (\"money\", \"NN\"), (\"market\", \"NN\"), (\"fund\", \"NN\")]\n",
    "sentence3 = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"), (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"), (\"the\", \"DT\"), (\"cat\", \"NN\"), (\"owner\", \"NN\")]\n",
    "\n",
    "tag_pattern2 = \"NP: {<DT>+<NN>+}\"\n",
    "\n",
    "import nltk \n",
    "NPAnotherChunker = nltk.RegexpParser(tag_pattern2) \n",
    "result1 = NPAnotherChunker.parse(sentence1) \n",
    "print(\"Result1: %s\" % result1)\n",
    "result2 = NPAnotherChunker.parse(sentence2) \n",
    "print(\"Result2: %s\" % result2)\n",
    "result3 = NPAnotherChunker.parse(sentence3) \n",
    "print(\"Result3: %s\" % result3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create a tag pattern that will chunk the determiner and any number of consecutive nouns. - call this `tag_pattern2`.  I.e. will chunk sentence1 into \"the money market fund\" and will not chunk sentence2 into any noun phrases.  (10 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Extract the information from the following text and create a structured dataFrame with column names `Name` and `Place`. (25 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Place</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jason</td>\n",
       "      <td>Chicago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>James</td>\n",
       "      <td>Washington</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Robert</td>\n",
       "      <td>Boston</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Michael</td>\n",
       "      <td>Denver</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Name       Place\n",
       "0    Jason     Chicago\n",
       "1    James  Washington\n",
       "2   Robert      Boston\n",
       "3  Michael      Denver"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Jason is from Chicago, James is from Washington, Robert is from Boston and Michael is from Denver\"\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "english_model = spacy.load('en')\n",
    "d = english_model(text)\n",
    "names = [entity.text for entity in list(d.ents) if entity.label_ in ['PERSON']]\n",
    "gpe = [entity.text for entity in list(d.ents) if entity.label_ in ['GPE']]\n",
    "\n",
    "df=pd.DataFrame({\"Name\":np.array(names), \"Place\":np.array(gpe)})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Consider the following piece of text and extract the relationship between Person(PER) and Place(GPE). (30 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = \"Tom is from Illinois, Joseph is from Texas and Penny is from Omaha which is in Nebraska\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person: ['Tom'], Place: ['Illinois']\n",
      "Person: ['Joseph'], Place: ['Texas']\n",
      "Person: ['Penny'], Place: ['Omaha', 'Nebraska']\n"
     ]
    }
   ],
   "source": [
    "test = \"Tom is from Illinois, Joseph is from Texas and Penny is from Omaha which is in Nebraska\"\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk \n",
    "\n",
    "# english_model = spacy.load('en')\n",
    "# d = english_model(test)\n",
    "# names = [entity.text for entity in list(d.ents) if entity.label_ in ['PERSON']]\n",
    "# gpe = [entity.text for entity in list(d.ents) if entity.label_ in ['GPE']]\n",
    "\n",
    "# #print relations\n",
    "# for i in range(len(gpe)):\n",
    "#     if i>=len(names):\n",
    "#         j=len(names)-1\n",
    "#     else:\n",
    "#         j=i\n",
    "#     print(\"%s is from %s\" % (names[j],gpe[i]))\n",
    "\n",
    "#Tokenize/tag and chunk input    \n",
    "tagged=nltk.pos_tag(nltk.word_tokenize(test))\n",
    "tag_pattern = \"NP: {<NNP>+<.*>?<IN>+<NNP>+(<WDT><.*>+)?}\"\n",
    "NPAnotherChunker = nltk.RegexpParser(tag_pattern) \n",
    "result = NPAnotherChunker.parse(tagged)\n",
    "#print(\"Result: %s\" % result)\n",
    "\n",
    "#Iterate through NP chunks and connect entities based on type\n",
    "df=pd.DataFrame({\"Person\":[], \"Place\":[]})\n",
    "subtrees = [st for st in result.subtrees()]\n",
    "\n",
    "english_model = spacy.load('en')\n",
    "for i in range(1,len(subtrees)):\n",
    "    s = subtrees[i].__str__().split(\" \")\n",
    "    newString = \"\"\n",
    "    #build new string without tags\n",
    "    for j in range(1,len(s)):\n",
    "        tmp = s[j].split(\"/\")[0]\n",
    "        if tmp==\"\":\n",
    "            continue\n",
    "        newString = newString + \" \" + tmp    \n",
    "    \n",
    "    #Extract ents/connect ents for each chunk\n",
    "    d = english_model(newString)\n",
    "    names = [entity.text for entity in list(d.ents) if entity.label_ in ['PERSON']]\n",
    "    gpe = [entity.text for entity in list(d.ents) if entity.label_ in ['GPE']]\n",
    "    \n",
    "    print(\"Person: %s, Place: %s\" % (names,gpe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
